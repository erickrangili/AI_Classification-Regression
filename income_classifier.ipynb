{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tiger\\Anaconda3\\envs\\Artificial Intelligence\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Tiger\\Anaconda3\\envs\\Artificial Intelligence\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Tiger\\Anaconda3\\envs\\Artificial Intelligence\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Tiger\\Anaconda3\\envs\\Artificial Intelligence\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 70.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tiger\\Anaconda3\\envs\\Artificial Intelligence\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn import model_selection\n",
    "\n",
    "# Input file containing data\n",
    "input_file = 'income_data.txt'\n",
    "\n",
    "# In order to load the data from the file, we need to preprocess it so that we can prepare it for\n",
    "# classification. We will use at most 25,000 data points for each class:\n",
    "# Read the data\n",
    "X = []\n",
    "y = []\n",
    "count_class1 = 0\n",
    "count_class2 = 0\n",
    "max_datapoints = 25000\n",
    "\n",
    "# Open the file and start reading the lines:\n",
    "with open(input_file, 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        if count_class1 >= max_datapoints and count_class2 >=max_datapoints:\n",
    "            break\n",
    "            \n",
    "        if '?' in line:\n",
    "            continue\n",
    "            \n",
    "        # Each line is comma separated, so we need to split it accordingly. The last element in each\n",
    "        # line represents the label. Depending on that label, we will assign it to a class:\n",
    "\n",
    "        data = line[:-1].split(', ')\n",
    "\n",
    "        if data[-1] == '<=50K' and count_class1 < max_datapoints:\n",
    "            X.append(data)\n",
    "            count_class1 += 1\n",
    "    \n",
    "        if data[-1] == '>50K' and count_class2 < max_datapoints:\n",
    "            X.append(data)\n",
    "            count_class2 += 1\n",
    "    \n",
    "# Convert the list into a numpy array so that we can give it as an input to the sklearn function:\n",
    "# Convert to numpy array\n",
    "X = np.array(X)\n",
    "\n",
    "# If any attribute is a string, then we need to encode it. If it is a number, we can keep it as it is.\n",
    "# Note that we will end up with multiple label encoders and we need to keep track of all of them:\n",
    "\n",
    "# Convert string data to numerical data\n",
    "label_encoder = []\n",
    "X_encoded = np.empty(X.shape)\n",
    "for i,item in enumerate(X[0]):\n",
    "    if item.isdigit():\n",
    "        X_encoded[:, i] = X[:, i]\n",
    "    else:\n",
    "        label_encoder.append(preprocessing.LabelEncoder())\n",
    "        X_encoded[:, i] = label_encoder[-1].fit_transform(X[:, i])\n",
    "        \n",
    "X = X_encoded[:, :-1].astype(int)\n",
    "y = X_encoded[:, -1].astype(int)\n",
    "\n",
    "\n",
    "# Create SVM classifier with a linear kernel\n",
    "classifier = OneVsOneClassifier(LinearSVC(random_state=0))\n",
    "\n",
    "# Train the classifier\n",
    "classifier.fit(X, y)\n",
    "\n",
    "# Perform cross validation using an 80/20 split for training and testing, and then predict the output for training data:\n",
    "# Cross validation\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y,test_size=0.2, random_state=5)\n",
    "classifier = OneVsOneClassifier(LinearSVC(random_state=0))\n",
    "classifier.fit(X_train, y_train)\n",
    "y_test_pred = classifier.predict(X_test)\n",
    "\n",
    "\n",
    "# Compute the F1 score of the SVM classifier\n",
    "f1 = model_selection.cross_val_score(classifier, X, y,scoring='f1_weighted', cv=3)\n",
    "print(\"F1 score: \" + str(round(100*f1.mean(), 2)) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "bad input shape ()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-0cbf6c6a1fe3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0minput_data_encoded\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0minput_data_encoded\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel_encoder\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0mcount\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Artificial Intelligence\\lib\\site-packages\\sklearn\\preprocessing\\label.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    250\u001b[0m         \"\"\"\n\u001b[0;32m    251\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'classes_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 252\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    253\u001b[0m         \u001b[1;31m# transform of empty array is empty array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Artificial Intelligence\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcolumn_or_1d\u001b[1;34m(y, warn)\u001b[0m\n\u001b[0;32m    758\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    759\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 760\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"bad input shape {0}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    762\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: bad input shape ()"
     ]
    }
   ],
   "source": [
    "# Now that the classifier is ready, let's see how to take a random input data point and predict\n",
    "# the output. Let's define one such data point:\n",
    "# Predict output for a test datapoint\n",
    "\n",
    "input_data = ['37', 'Private', '215646', 'HS-grad', '9', 'Never-married','Handlers-cleaners', 'Not-in-family', 'White', 'Male', '0', '0', '40','United-States']\n",
    "\n",
    "# Before we can perform prediction, we need to encode this data point using the label encoders we created earlier:\n",
    "# Encode test datapoint\n",
    "\n",
    "input_data_encoded = [-1] * len(input_data)\n",
    "count = 0\n",
    "for i, item in enumerate(input_data):\n",
    "    if item.isdigit():\n",
    "        input_data_encoded[i] = int(input_data[i])\n",
    "    else:\n",
    "        input_data_encoded[i] = int(label_encoder[count].transform(input_data[i]))\n",
    "        count += 1\n",
    "\n",
    "input_data_encoded = np.array(input_data_encoded)\n",
    "\n",
    "# We are now ready to predict the output using the classifier:\n",
    "# Run classifier on encoded datapoint and print output\n",
    "predicted_class = classifier.predict(input_data_encoded)\n",
    "print(label_encoder[-1].inverse_transform(predicted_class)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
